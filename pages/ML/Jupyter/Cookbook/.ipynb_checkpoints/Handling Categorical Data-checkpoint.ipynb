{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "protected-touch",
   "metadata": {},
   "source": [
    "**Basics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-player",
   "metadata": {},
   "source": [
    "Sets of categories with no intrinsic ordering is\n",
    "called **nominal**. Examples of nominal categories include:\n",
    "• Blue, Red, Green\n",
    "\n",
    "• Man, Woman\n",
    "\n",
    "• Banana, Strawberry, Apple\n",
    "\n",
    "In contrast, when a set of categories has some natural ordering we refer to it as ordinal.\n",
    "For example:\n",
    "• Low, Medium, High\n",
    "\n",
    "• Young, Old\n",
    "\n",
    "• Agree, Neutral, Disagree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "african-contents",
   "metadata": {},
   "source": [
    "**Encoding Nominal Categorical Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "optical-assurance",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "trying-signal",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = np.array([[\"Texas\"],\n",
    "[\"California\"],\n",
    "[\"Texas\"],\n",
    "[\"Delaware\"],\n",
    "[\"Texas\"]])\n",
    "# Create one-hot encoder\n",
    "one_hot = LabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "modified-lebanon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot.fit_transform(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "literary-missile",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['California', 'Delaware', 'Texas'], dtype='<U10')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View feature classes\n",
    "one_hot.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chicken-agenda",
   "metadata": {},
   "source": [
    "If we want to reverse the one-hot encoding, we can use inverse_transform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "weighted-bryan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Texas', 'California', 'Texas', 'Delaware', 'Texas'], dtype='<U10')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot.inverse_transform(one_hot.transform(feature))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greenhouse-royalty",
   "metadata": {},
   "source": [
    "We can even use pandas to one-hot encode the feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "valuable-equity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>California</th>\n",
       "      <th>Delaware</th>\n",
       "      <th>Texas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   California  Delaware  Texas\n",
       "0           0         0      1\n",
       "1           1         0      0\n",
       "2           0         0      1\n",
       "3           0         1      0\n",
       "4           0         0      1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import library\n",
    "import pandas as pd\n",
    "# Create dummy variables from feature\n",
    "pd.get_dummies(feature[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "difficult-dietary",
   "metadata": {},
   "source": [
    "One helpful ability of scikit-learn is to handle a situation where each observation lists\n",
    "multiple classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "binary-mexico",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multiclass feature\n",
    "multiclass_feature = [(\"Texas\", \"Florida\"),\n",
    "(\"California\", \"Alabama\"),\n",
    "(\"Texas\", \"Florida\"),\n",
    "(\"Delware\", \"Florida\"),\n",
    "(\"Texas\", \"Alabama\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "loved-nickel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multiclass one-hot encoder\n",
    "one_hot_multiclass = MultiLabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "numerous-inquiry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 1],\n",
       "       [1, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 1],\n",
       "       [0, 0, 1, 1, 0],\n",
       "       [1, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_multiclass.fit_transform(multiclass_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "brazilian-proposal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Alabama', 'California', 'Delware', 'Florida', 'Texas'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_multiclass.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corporate-carroll",
   "metadata": {},
   "source": [
    "Finally, it is worthwhile to note that it is often recommended that after one-hot\n",
    "encoding a feature, we drop one of the one-hot encoded features in the resulting\n",
    "matrix to avoid linear dependence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respected-plenty",
   "metadata": {},
   "source": [
    "**Encoding Ordinal Categorical Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "described-layout",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Create features\n",
    "dataframe = pd.DataFrame({\"Score\": [\"Low\", \"Low\", \"Medium\", \"Medium\", \"High\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "detailed-breeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mapper\n",
    "scale_mapper = {\"Low\":1,\n",
    "\"Medium\":2,\n",
    "\"High\":3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "medium-infection",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    2\n",
       "3    2\n",
       "4    3\n",
       "Name: Score, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace feature values with scale\n",
    "dataframe[\"Score\"].replace(scale_mapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electrical-cursor",
   "metadata": {},
   "source": [
    "Often we have a feature with classes that have some kind of natural ordering. A\n",
    "famous example is the Likert scale:\n",
    "• Strongly Agree\n",
    "\n",
    "• Agree\n",
    "\n",
    "• Neutral\n",
    "\n",
    "• Disagree\n",
    "\n",
    "• Strongly Disagree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generous-springer",
   "metadata": {},
   "source": [
    "In this example, the distance between Low and Medium is the same as the distance\n",
    "between Medium and Barely More Than Medium, which is almost certainly not accurate.\n",
    "The best approach is to be conscious about the numerical values mapped to\n",
    "classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "impressed-description",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_mapper = {\"Low\":1,\n",
    "\"Medium\":2,\n",
    "\"Barely More Than Medium\": 2.1,\n",
    "\"High\":3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "vocal-georgia",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame({\"Score\": [\"Low\",\n",
    "\"Low\",\n",
    "\"Medium\",\n",
    "\"Medium\",\n",
    "\"High\",\n",
    "\"Barely More Than Medium\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "executive-allocation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1.0\n",
       "1    1.0\n",
       "2    2.0\n",
       "3    2.0\n",
       "4    3.0\n",
       "5    2.1\n",
       "Name: Score, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe[\"Score\"].replace(scale_mapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olive-mailman",
   "metadata": {},
   "source": [
    "**Encoding Dictionaries of Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "elegant-twist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "senior-essex",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = [{\"Red\": 2, \"Blue\": 4},\n",
    "{\"Red\": 4, \"Blue\": 3},\n",
    "{\"Red\": 1, \"Yellow\": 2},\n",
    "{\"Red\": 2, \"Yellow\": 2}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dominican-penguin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary vectorizer\n",
    "dictvectorizer = DictVectorizer(sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bronze-conference",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dictionary to feature matrix\n",
    "features = dictvectorizer.fit_transform(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "amazing-irish",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4., 2., 0.],\n",
       "       [3., 4., 0.],\n",
       "       [0., 1., 2.],\n",
       "       [0., 2., 2.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-colombia",
   "metadata": {},
   "source": [
    "By default DictVectorizer outputs a sparse matrix that only stores elements with a\n",
    "value other than 0. This can be very helpful when we have massive matrices (often\n",
    "encountered in natural language processing) and want to minimize the memory\n",
    "requirements. We can force DictVectorizer to output a dense matrix using\n",
    "sparse=False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "resistant-benjamin",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = dictvectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "executive-fundamentals",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Blue', 'Red', 'Yellow']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ultimate-piece",
   "metadata": {},
   "source": [
    "**Imputing Missing Class Values**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executive-conservative",
   "metadata": {},
   "source": [
    "The ideal solution is to train a machine learning classifier algorithm to predict the\n",
    "missing values, commonly a k-nearest neighbors (KNN) classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "narrative-acoustic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "conditional-effects",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature matrix with categorical feature\n",
    "X = np.array([[0, 2.10, 1.45],\n",
    "[1, 1.18, 1.33],\n",
    "[0, 1.22, 1.27],\n",
    "[1, -0.21, -1.19]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "mounted-bahamas",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_with_nan = np.array([[np.nan, 0.87, 1.31],\n",
    "[np.nan, -0.67, -0.22]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "treated-youth",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier(3, weights='distance')\n",
    "trained_model = clf.fit(X[:,1:], X[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "north-giving",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_values = trained_model.predict(X_with_nan[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "possible-fundamental",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_with_imputed = np.hstack((imputed_values.reshape(-1,1), X_with_nan[:,1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "sealed-journalist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.  ,  0.87,  1.31],\n",
       "       [ 1.  , -0.67, -0.22],\n",
       "       [ 0.  ,  2.1 ,  1.45],\n",
       "       [ 1.  ,  1.18,  1.33],\n",
       "       [ 0.  ,  1.22,  1.27],\n",
       "       [ 1.  , -0.21, -1.19]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.vstack((X_with_imputed, X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "velvet-kazakhstan",
   "metadata": {},
   "source": [
    "When we have missing values in a categorical feature, our best solution is to open our\n",
    "toolbox of machine learning algorithms to predict the values of the missing observations.\n",
    "We can accomplish this by treating the feature with the missing values as the\n",
    "target vector and the other features as the feature matrix. A commonly used algorithm\n",
    "is KNN (discussed in depth later in this book), which assigns to the missing\n",
    "value the median class of the k nearest observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deadly-sampling",
   "metadata": {},
   "source": [
    "**Handling Imbalanced Classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "material-target",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "peaceful-stable",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load iris data\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "occasional-specific",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature matrix\n",
    "features = iris.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "parliamentary-colombia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target vector\n",
    "target = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "premium-benjamin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove first 40 observations\n",
    "features = features[40:,:]\n",
    "target = target[40:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "obvious-excerpt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary target vector indicating if class 0\n",
    "target = np.where((target == 0), 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "lyric-mining",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifth-boards",
   "metadata": {},
   "source": [
    "Many algorithms in scikit-learn offer a parameter to weight classes during training to\n",
    "counteract the effect of their imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "smaller-mitchell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create weights\n",
    "weights = {0: .9, 1: 0.1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "former-handling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(class_weight={0: 0.9, 1: 0.1})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create random forest classifier with weights\n",
    "RandomForestClassifier(class_weight=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abstract-baseball",
   "metadata": {},
   "source": [
    "Or you can pass balanced, which automatically creates weights inversely proportional\n",
    "to class frequencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "honey-mississippi",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(class_weight='balanced')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a random forest with balanced class weights\n",
    "RandomForestClassifier(class_weight=\"balanced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documentary-least",
   "metadata": {},
   "source": [
    "Alternatively, we can downsample the majority class or upsample the minority class.\n",
    "In downsampling, we randomly sample without replacement from the majority class\n",
    "(i.e., the class with more observations) to create a new subset of observations equal in\n",
    "size to the minority class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valuable-article",
   "metadata": {},
   "source": [
    "For example, if the minority class has 10 observations, we\n",
    "will randomly select 10 observations from the majority class and use those 20 observations\n",
    "as our data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "undefined-myanmar",
   "metadata": {},
   "source": [
    "In the real world, imbalanced classes are everywhere—most visitors don’t click the\n",
    "buy button and many types of cancer are thankfully rare. For this reason, handling\n",
    "imbalanced classes is a common activity in machine learning.\n",
    "Our best strategy is simply to collect more observations—especially observations\n",
    "from the minority class. However, this is often just not possible, so we have to resort\n",
    "to other options.\n",
    "A second strategy is to use a model evaluation metric better suited to imbalanced\n",
    "classes. Accuracy is often used as a metric for evaluating the performance of a model,\n",
    "but when imbalanced classes are present accuracy can be ill suited. For example, if\n",
    "only 0.5% of observations have some rare cancer, then even a naive model that\n",
    "predicts nobody has cancer will be 99.5% accurate. Clearly this is not ideal. Some better\n",
    "metrics we discuss in later chapters are confusion matrices, precision, recall, F1\n",
    "scores, and ROC curves.\n",
    "A third strategy is to use the class weighing parameters included in implementations\n",
    "of some models. This allows us to have the algorithm adjust for imbalanced classes.\n",
    "Fortunately, many scikit-learn classifiers have a class_weight parameter, making it a\n",
    "good option.\n",
    "\n",
    "The fourth and fifth strategies are related: downsampling and upsampling. In downsampling\n",
    "we create a random subset of the majority class of equal size to the minority\n",
    "class. In upsampling we repeatedly sample with replacement from the minority class\n",
    "to make it of equal size as the majority class. The decision between using downsampling\n",
    "and upsampling is context-specific, and in general we should try both to see\n",
    "which produces better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competent-stick",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
